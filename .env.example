# ============================================================================
# RAG LLM Application - Environment Configuration
# ============================================================================
# Copy this file to .env and fill in your actual credentials
# Never commit .env to version control!
# ============================================================================

# ============================================================================
# MODEL BACKEND SELECTION
# ============================================================================
# Choose which backend to use for embeddings and text generation:
# - "local"  : Use local sentence-transformers + LLM (free, requires GPU)
# - "api"    : Use OpenAI embeddings and GPT (requires API key)
# - "gemini" : Use Google Gemini API (recommended for quick start)
#
MODEL_BACKEND=gemini

# ============================================================================
# GOOGLE GEMINI API CONFIGURATION (for MODEL_BACKEND=gemini)
# ============================================================================
# Get your free Gemini API key from: https://aistudio.google.com/app/apikey

# Your Gemini API Key (required when using gemini backend)
GEMINI_API_KEY=your_api_key_here

# Optional: Customize Gemini models (defaults shown below)
GEMINI_EMBEDDING_MODEL=models/embedding-001
GEMINI_EMBEDDING_DIMENSION=768
GEMINI_LLM_MODEL=gemini-2.0-flash

# ============================================================================
# OPENAI API CONFIGURATION (for MODEL_BACKEND=api)
# ============================================================================
# Only needed if using OpenAI backend
# Get your API key from: https://platform.openai.com/api-keys

# OPENAI_API_KEY=your_openai_api_key_here
# EMBEDDING_MODEL=text-embedding-3-small
# LLM_MODEL=gpt-4-turbo-preview

# ============================================================================
# SUPABASE CONFIGURATION (Optional - for vector database storage)
# ============================================================================
# Only needed if you want to store embeddings in a database
# Create free account at: https://supabase.com

# SUPABASE_URL=https://your-project.supabase.co
# SUPABASE_SERVICE_ROLE_KEY=your_service_role_key
# SUPABASE_TABLE_NAME=documents

# ============================================================================
# LOCAL MODEL CONFIGURATION (for MODEL_BACKEND=local)
# ============================================================================
# Only needed if using local models
# Requires CUDA GPU (8GB+ VRAM recommended)

# LOCAL_EMBED_MODEL=all-mpnet-base-v2
# LOCAL_LLM_MODEL=google/gemma-7b-it
# DEVICE=cuda  # or "cpu" if no GPU available

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================
# Logging level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# ============================================================================
# APPLICATION SETTINGS
# ============================================================================
# Maximum number of documents to retrieve for context
MAX_RETRIEVAL_RESULTS=5

# Minimum similarity score threshold for document retrieval
SIMILARITY_THRESHOLD=0.5

# Maximum tokens for LLM responses
MAX_RESPONSE_TOKENS=512

# Temperature for text generation (0.0-1.0, higher = more creative)
GENERATION_TEMPERATURE=0.7

# ============================================================================
# NOTES
# ============================================================================
# 1. Start with MODEL_BACKEND=gemini for the easiest setup
# 2. Get your Gemini API key from: https://aistudio.google.com/app/apikey
# 3. Install dependencies: pip install -r requirements.txt
# 4. Test setup: python test_gemini_integration.py
# 5. See GEMINI_SETUP_GUIDE.md for detailed Gemini setup instructions
# ============================================================================
